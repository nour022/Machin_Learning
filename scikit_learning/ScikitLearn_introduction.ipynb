{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa47ce2d-c765-4cef-b6ac-ec5a541b756d",
   "metadata": {},
   "source": [
    "# Was wir in der Einführung zu Scikit-Learn behandeln\r\n",
    "\r\n",
    "Dieses Notebook umreißt den Inhalt, der in der Einführung zu Scikit-Learn behandelt wird.\r\n",
    "\r\n",
    "Es ist ein schneller Überblick über alle Scikit-Learn Funktionen und Module für jede beschriebene Sektion.\r\n",
    "\r\n",
    "Das, was wir behandeln, folgt dem folgenden Diagramm, das einen Scikit-Learn Arbeitsablauf detailliert darstellt.\r\n",
    "\r\n",
    "<img src=\"../images/sklearn-workflow-title.png\"/>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7196d5-2db3-43b9-896a-3e74575da235",
   "metadata": {},
   "source": [
    "## 0. Importieren der Standardbibliotheken\r\n",
    "\r\n",
    "In vielen Machine-Learning-Projekten werden oft diese Bibliotheken (Matplotlib, NumPy und pandas) zu Beginn importiert.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7eb243b2-a67e-4277-acc2-3a27744fde65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20fffcc3-ef9c-4344-9699-0b1a4e78e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data\n",
    "heart_disease = pd.read_csv(\"../data/heart-disease.csv\") ## get  a data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15a5325d-daf1-4852-bf70-19f7e2f75e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = heart_disease.drop(\"target\",axis=1) # split out the target colum \n",
    "y = heart_disease[\"target\"]  # get only the target colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a0ecbbe-c24f-497f-acf7-4bc5cb54749e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize RandomForestClassifier with default hyperparameters\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "# clf = RandomForestClassifier()\n",
    "\n",
    "# Retrieve the default hyperparameters of the classifier\n",
    "default_params = clf.get_params()\n",
    "default_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a65f9b-c166-421b-b7f3-043a87bb552f",
   "metadata": {},
   "source": [
    "## from sklearn.model_selection import train_test_split\n",
    "\n",
    "die verwendet wird, um einen Datensatz in Trainings- und Testsets aufzuteilen. \n",
    "Dies ist eine grundlegende Methode, um die Leistung eines Machine-Learning-Modells zu bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c714772f-c579-4b77-8a14-f5f44cc2d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fit the model to the training data \n",
    "from sklearn.model_selection import train_test_split\n",
    "# Diese Daten werden für das Training des Modells verwendet\n",
    "# test_size=0.2 bedeutet, dass 20 % der Daten für das Testen verwendet werden, während 80 % für das Training verwendet werden\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5f94ad81-77ec-4a29-a3a6-173a26d8c18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], dtype=int64),\n",
       " array([[0.82222222, 0.17777778],\n",
       "        [0.93333333, 0.06666667],\n",
       "        [0.21111111, 0.78888889],\n",
       "        [0.26666667, 0.73333333],\n",
       "        [0.31111111, 0.68888889],\n",
       "        [0.07777778, 0.92222222],\n",
       "        [0.7       , 0.3       ],\n",
       "        [0.32222222, 0.67777778],\n",
       "        [0.56666667, 0.43333333],\n",
       "        [0.06666667, 0.93333333],\n",
       "        [0.78888889, 0.21111111],\n",
       "        [0.88888889, 0.11111111],\n",
       "        [0.75555556, 0.24444444],\n",
       "        [0.21111111, 0.78888889],\n",
       "        [0.65555556, 0.34444444],\n",
       "        [0.42222222, 0.57777778],\n",
       "        [0.3       , 0.7       ],\n",
       "        [0.78888889, 0.21111111],\n",
       "        [0.76666667, 0.23333333],\n",
       "        [0.5       , 0.5       ],\n",
       "        [0.37777778, 0.62222222],\n",
       "        [0.85555556, 0.14444444],\n",
       "        [0.52222222, 0.47777778],\n",
       "        [0.24444444, 0.75555556],\n",
       "        [0.07777778, 0.92222222],\n",
       "        [0.74444444, 0.25555556],\n",
       "        [0.42222222, 0.57777778],\n",
       "        [0.07777778, 0.92222222],\n",
       "        [0.95555556, 0.04444444],\n",
       "        [0.34444444, 0.65555556],\n",
       "        [0.65555556, 0.34444444],\n",
       "        [0.42222222, 0.57777778],\n",
       "        [0.32222222, 0.67777778],\n",
       "        [0.98888889, 0.01111111],\n",
       "        [0.07777778, 0.92222222],\n",
       "        [0.03333333, 0.96666667],\n",
       "        [0.05555556, 0.94444444],\n",
       "        [0.34444444, 0.65555556],\n",
       "        [0.97777778, 0.02222222],\n",
       "        [0.03333333, 0.96666667],\n",
       "        [0.3       , 0.7       ],\n",
       "        [0.94444444, 0.05555556],\n",
       "        [0.93333333, 0.06666667],\n",
       "        [0.78888889, 0.21111111],\n",
       "        [0.14444444, 0.85555556],\n",
       "        [0.65555556, 0.34444444],\n",
       "        [0.14444444, 0.85555556],\n",
       "        [0.43333333, 0.56666667],\n",
       "        [0.17777778, 0.82222222],\n",
       "        [0.44444444, 0.55555556],\n",
       "        [0.02222222, 0.97777778],\n",
       "        [0.61111111, 0.38888889],\n",
       "        [0.95555556, 0.04444444],\n",
       "        [0.14444444, 0.85555556],\n",
       "        [0.97777778, 0.02222222],\n",
       "        [0.6       , 0.4       ],\n",
       "        [0.05555556, 0.94444444],\n",
       "        [0.96666667, 0.03333333],\n",
       "        [0.74444444, 0.25555556],\n",
       "        [0.6       , 0.4       ],\n",
       "        [0.53333333, 0.46666667]]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modell mit Trainingsdaten trainieren\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen auf dem Testdatensatz machen\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Vorhersagen mit Wahrscheinlichkeiten treffen (für Klassifikationsmodelle)\n",
    "y_probs = clf.predict_proba(X_test)\n",
    "y_preds, y_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9df86-93ce-4a16-9d98-5efd050d8074",
   "metadata": {},
   "source": [
    "# 4. Evaluierung des Modells\r\n",
    "\r\n",
    "Jedes Modell in Scikit-Learn verfügt über eine Standardmetrik, die über die Funktion `score()` zugänglich ist.\r\n",
    "\r\n",
    "Es gibt jedoch eine Vielzahl verschiedener Evaluierungsmetriken, die je nach verwendetem Modell verwendet werden können.\r\n",
    "\r\n",
    "Eine vollständige Liste der Evaluierungsmetriken kann [in der Dokumentation](https://scikit-learn.org/stable/modules/model_evaluation.html) gefunden wrden.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c214a-8eef-476c-88ce-bf549703a18c",
   "metadata": {},
   "source": [
    "### score(): \n",
    "Der score()-Befehl berechnet die Genauigkeit des Modells auf den Trainingsdaten, indem es die Vorhersagen des Modells auf den Trainingsdaten macht und diese mit den tatsächlichen Trainingslabels vergleicht. Für Klassifikationsmodelle ist die Genauigkeit das Verhältnis der korrekt vorhergesagten Instanzen zur Gesamtanzahl der Instanzen im Trainingsdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "55dc5030-28c7-4a02-b15c-2e6fb0b2f5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18365c75-82de-428d-9ff6-7836f10ec35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8852459016393442"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ff603f6b-98ac-441f-9509-dc9629f14733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78688525 0.85245902 0.81967213 0.8        0.81666667]\n",
      "[0.80555556 0.93333333 0.84375    0.84848485 0.76923077]\n"
     ]
    }
   ],
   "source": [
    "# Die Evaluierung eines Modells mit Kreuzvalidierung ist möglich mit cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# scoring=None bedeutet, dass die Standard-Score-Metrik score() verwendet wird\n",
    "print(cross_val_score(estimator=clf, \n",
    "                      X=x, \n",
    "                      y=y, \n",
    "                      cv=5, # Verwendung von 5-facher Kreuzvalidierung\n",
    "                      scoring=None)) \n",
    "\n",
    "# Evaluierung eines Modells mit einer anderen Bewertungsmethode\n",
    "print(cross_val_score(estimator=clf, \n",
    "                      X=x, \n",
    "                      y=y,\n",
    "                      cv=5, # Verwendung von 5-facher Kreuzvalidierung\n",
    "                      scoring=\"precision\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d5ed8e71-cc2b-4fc8-a1d8-70a78880fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8852459016393442\n",
      "0.8849462365591398\n",
      "[[26  4]\n",
      " [ 3 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88        30\n",
      "           1       0.88      0.90      0.89        31\n",
      "\n",
      "    accuracy                           0.89        61\n",
      "   macro avg       0.89      0.88      0.89        61\n",
      "weighted avg       0.89      0.89      0.89        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Different classification metrics\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "\n",
    "# Reciver Operating Characteristic (ROC curve)/Area under curve (AUC)\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probs[:, 1])\n",
    "print(roc_auc_score(y_test, y_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_preds))\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2965df48-326c-4b54-bd05-9822067b882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44433066666666665\n",
      "0.27475409836065573\n",
      "0.1344\n"
     ]
    }
   ],
   "source": [
    "# Different regression metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "boston_df = heart_disease\n",
    "# Make predictions first\n",
    "X = boston_df.drop(\"target\", axis=1)\n",
    "y = boston_df[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# R^2 (pronounced r-squared) or coefficient of determination\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_test, y_preds))\n",
    "\n",
    "# Mean absolute error (MAE)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(y_test, y_preds))\n",
    "\n",
    "# Mean square error (MSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3cb12-d1f8-4485-9a70-ec2edf86c4fc",
   "metadata": {},
   "source": [
    "## 5. Verbesserung durch Experimente\n",
    "\n",
    "Zwei der Hauptmethoden, um die Ausgangsmetriken eines Modells zu verbessern (die ersten Bewertungsmetriken, die Sie erhalten).\n",
    "\n",
    "Von einem Datenperspektive aus betrachtet:\n",
    "* Könnten wir mehr Daten sammeln? Im maschinellen Lernen sind mehr Daten im Allgemeinen besser, da sie einem Modell mehr Möglichkeiten bieten, Muster zu lernen.\n",
    "* Könnten wir unsere Daten verbessern? Dies könnte bedeuten, fehlende Werte zu ergänzen oder eine bessere Kodierungsstrategie (Umformung von Dingen in Zahlen) zu finden.\n",
    "\n",
    "Von einem Modellperspektive aus betrachtet:\n",
    "* Gibt es ein besseres Modell, das wir verwenden könnten? Wenn Sie mit einem einfachen Modell begonnen haben, könnten Sie ein komplexeres verwenden? (Wir haben ein Beispiel dafür gesehen, als wir uns die [Scikit-Learn-Maschinenlernkarte](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) angesehen haben - Ensemble-Methoden gelten im Allgemeinen als komplexere Modelle).\n",
    "* Könnten wir das aktuelle Modell verbessern? Wenn das Modell, das Sie verwenden, bereits gut funktioniert, können die **Hyperparameter** eingestellt werden, um es noch besser zu machen?\n",
    "\n",
    "**Hyperparameter** sind wie Einstellungen an einem Modell, die Sie anpassen können, damit einige der Methoden, die es zum Finden von Mustern verwendet, verändert und potenziell verbessert werden. Die Anpassung von Hyperparametern wird als Hyperparameter-Tuning bezeichnet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c338fbba-bf0a-4904-b056-f585b88fd87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to find a model's hyperparameters\n",
    "clf = RandomForestClassifier()\n",
    "clf.get_params() # returns a list of adjustable hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b9817695-8dab-44a1-bb56-7fbc39af3729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying mpdel with 10 n_estimators\n",
      "Model accuracy on test set: 80.33%\n",
      "Trying mpdel with 20 n_estimators\n",
      "Model accuracy on test set: 86.89%\n",
      "Trying mpdel with 30 n_estimators\n",
      "Model accuracy on test set: 80.33%\n",
      "Trying mpdel with 40 n_estimators\n",
      "Model accuracy on test set: 78.69%\n",
      "Trying mpdel with 50 n_estimators\n",
      "Model accuracy on test set: 83.61%\n",
      "Trying mpdel with 60 n_estimators\n",
      "Model accuracy on test set: 80.33%\n",
      "Trying mpdel with 70 n_estimators\n",
      "Model accuracy on test set: 83.61%\n",
      "Trying mpdel with 80 n_estimators\n",
      "Model accuracy on test set: 81.97%\n",
      "Trying mpdel with 90 n_estimators\n",
      "Model accuracy on test set: 81.97%\n"
     ]
    }
   ],
   "source": [
    "# 5. Improve a model \n",
    "# Try different amount of n_estimators\n",
    "np.random.seed(10)\n",
    "for i in range(10,100,10):\n",
    "    print(f\"Trying mpdel with {i} n_estimators\")\n",
    "    clf = RandomForestClassifier(n_estimators=i).fit(X_train,y_train)\n",
    "    print(f\"Model accuracy on test set: {clf.score(X_test,y_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "18e9e79b-32bc-43ab-b8ed-1702df4ce6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# clf = RandomForestClassifier(n_estimators=30).fit(X_train,y_train)\n",
    "# Annahme: 'clf' ist das Modell, das gespeichert werden soll\n",
    "# Speichern des Modells in einer Datei mit 'pickle'\n",
    "with open(\"rs_random_forest_model_1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0fd2fa37-e2f9-4ec8-95fc-abf92344e7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.9672131147541"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a saved pickle model\n",
    "loaded_pickle_model = pickle.load(open(\"rs_random_forest_model_1.pkl\", \"rb\"))\n",
    "\n",
    "# Evaluate loaded model\n",
    "loaded_pickle_model.score(X_test, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829800e0-d279-4967-8d54-7378046b3b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
